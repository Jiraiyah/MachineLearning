{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NEURAL_NETWORKS.jpg)\n",
    "\n",
    "# Simple MNIST\n",
    "## Done By : Alireza Khodakarami\n",
    "\n",
    "In this note book we learn the very basics of a **neural network**, how we can prepare the data, how we can model our network, train and evaluate it. In more serious projects, you may want to break this process into multiple modules or note books, but for something as simple as mnist, we can hold everythin in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Normal base imports you would need almost in every neural network project, from tensorflow and it's functions, to matplot lib and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, LeakyReLU, LSTM\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.nn import relu, softmax\n",
    "from tensorflow.keras.utils import normalize\n",
    "#from sklearn.metrics import \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "\n",
    "We read the mnist data provided by tensorflow keras itself. Before tensor flow integrate these data inside themselves, we had to pip install mnist (search google for that if you need it), but with tensorflow providing the data internally, we don't need to install anything. Simply read the data and call load_data() function to get the two tuples with two lists in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Size of:')\n",
    "print(f'- Training-set:\\t\\t{len(x_train)}')\n",
    "print(f'- Test-set:\\t\\t{len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an example data\n",
    "\n",
    "It's always a good habit to print a sample as you go forward specially in data preparation and pre processing these data. At each step you print a sample from data to see where you are and what you need to do. Maybe when the whole project came together and you are done with the project, you would delete these printings but, when you go forward, it's a good habit to keep an eye on your data step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 100\n",
    "print(x_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[index], cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "Neural network, regardless of it's type, or the library you use it's API to train the neural, loves the data to sit between 0 to 1 or if needed -1 to 1. The pixel data we got is from 0 to 255. So? We need to rescale them down to fit 0 to 1 ratio. This rescaling process is called normalizaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, axis=1)\n",
    "x_test = normalize(x_test, axis=1)\n",
    "\n",
    "plt.imshow(x_train[index], cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(x_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Finally it's time for us to build our neural network model. I always separate each layer from the next one with an empty layer. People can argue that LeakyRelu is **A LAYER** by itself and technically they are right. But, in my eyes, it's still part of a hidden layer's functionality, AKA, **Activation**, so, I keep it with the other parts of that layer. Now, why put an empty line between layers? It makes it easier to identify layers and read and understand the model faster. I mean, you just need to look at the part bellow fast and when you got used to this style, you would already know we have three hidden layers and 300 nodes in each layer only in 2 seconds !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(300))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(300))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(300))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(10, activation = softmax))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model\n",
    "\n",
    "I don't know why they call the function **fit**, if you ask me, _train_ was a better name for this function. You can say that we are tailoring the model to FIT itself on the data but again, the machine is TEACHING itself using the data. Anyway, you can start the learning process with just one line. This process, can take few minutes up to days and weeks (depending on the number of epochs and the complexity of the model and also, the amount of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluation\n",
    "\n",
    "We get to a point that training our machine is done. What now? How would we know if the machine's algorithm is good or bad and to what degree? This is why we kept part of our data away for testing. Now, we test our machine, givint it new data that it never saw before, asking him to predict the result, compare his prediction with real result and give us the final evaluation on how good or bad the result of this comparison was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Loss : {val_loss}')\n",
    "print(f'Accuracy : {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving The Model\n",
    "\n",
    "Regardless of how good or bad your machine became, every now and then you should save your model to save your result. What if your computer crash and restart and you loose few days of job because you didn't save the model? Also, it's recommended that each time you save the model, you use a meaningful name that shows what model it is, in what structure it is, and when did it saved. Why? You will train tons of models, some will become better than others and some worse, how would you know which one of 50 models you have in one folder is the best you should choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Models/01 - MNIST.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Model\n",
    "\n",
    "Finally, this is the end goal of any neural network training process, to have a good model that they can load and use in future for their perpouse! This is how you would load a saved model and asign it to a variable to be called in any script later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./Models/01 - MNIST.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
